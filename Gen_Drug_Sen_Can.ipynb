{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37mNAlWNw7EL"
   },
   "source": [
    "<h2><b>1.</b> Import dataset and libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install category_encoders\n",
    "!pip install snapml\n",
    "!pip install xgboost\n",
    "!pip install lightgbm \n",
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"samiraalipour/genomics-of-drug-sensitivity-in-cancer-gdsc\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from snapml import SnapRandomForestRegressor, SnapBoostingMachineRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import skew, yeojohnson, zscore\n",
    "from scipy.stats.mstats import winsorize\n",
    "import time\n",
    "import gc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>2.</b> Load the Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(path)\n",
    "print(\"Files in the directory:\", files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lUipFUad1-BP",
    "outputId": "1e964b43-df87-4147-cc59-afbd28f01b0f"
   },
   "outputs": [],
   "source": [
    "csv_path = os.path.join(path, 'GDSC_DATASET.csv')\n",
    "print('Path to csv file:', csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "ILE-cVEGynEs",
    "outputId": "74f85357-d2d1-44bc-a0f9-e0ce2a416077"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZ__UY4e5P7h"
   },
   "source": [
    "For LN_IC50, AUC Z_SCORE: lower is better!\n",
    "The target is LN_IC50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uwy3obqFNBI-",
    "outputId": "9f91d460-dba5-4600-ec86-fcf895d85d28"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-3G6Hr1vQgs"
   },
   "source": [
    "<h2><b>3.</b> Data Preparation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JiYGCJWM7qK2",
    "outputId": "e94badf4-8013-40a9-878b-d237e787bba3"
   },
   "outputs": [],
   "source": [
    "duplicates = df.duplicated(keep='first')\n",
    "sum(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "id": "WfomuvsNflzd",
    "outputId": "55bce5ac-5750-4681-ae14-846f454ac794"
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bbhMY_wvWpV"
   },
   "source": [
    "<h3><b>3.1</b> Let's handle missing values!</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "iya3jeULaQv1",
    "outputId": "f76f38dd-18e9-449c-bb76-833ecc466ecf"
   },
   "outputs": [],
   "source": [
    "corr_matrix = df.select_dtypes(include=np.number).corr()\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Correlation Matrix (numerical only)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "id": "u_586jRgd0tS",
    "outputId": "8030a190-4fcb-465d-fa2f-2d716ee6866b"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WUHoAI9aOp8p",
    "outputId": "8692561d-3070-443e-eed6-7a1c0a0992ba"
   },
   "outputs": [],
   "source": [
    "df[\"Cancer Type (matching TCGA label)\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVe-aDj3qgos"
   },
   "source": [
    "The columns \"Cancer Type (matching TCGA label)\" and \"TCGA_DESC\" are essentially the same. I will first use them to replace each others missing values and then drop one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4IYb-Uk0Z4TY",
    "outputId": "7f20068a-62b1-458c-b335-02f622722415"
   },
   "outputs": [],
   "source": [
    "df.groupby(\"Cancer Type (matching TCGA label)\")[\"TCGA_DESC\"].agg([lambda x: list(x.unique()), 'nunique'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4xRBYnCAaMk3",
    "outputId": "f3333391-b126-47f9-d494-5567a8b5ea11"
   },
   "outputs": [],
   "source": [
    "df.groupby(\"TCGA_DESC\")[\"Cancer Type (matching TCGA label)\"].agg([lambda x: list(x.unique()), 'nunique'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9EjPTqXJeK6A"
   },
   "outputs": [],
   "source": [
    "df['Cancer Type (matching TCGA label)'] = df['Cancer Type (matching TCGA label)'].fillna(df['TCGA_DESC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHZCamcjkASl"
   },
   "outputs": [],
   "source": [
    "df['Cancer Type (matching TCGA label)'] = df['Cancer Type (matching TCGA label)'].replace(\"COREAD\", \"COAD/READ\")\n",
    "df['TCGA_DESC'] = df['TCGA_DESC'].replace(\"COREAD\", \"COAD/READ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Epb_8BRIl9-E"
   },
   "outputs": [],
   "source": [
    "df['Cancer Type (matching TCGA label)'] = df['Cancer Type (matching TCGA label)'].replace(\"UNABLE TO CLASSIFY\", \"UNCLASSIFIED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "507sWL9kpSNY"
   },
   "outputs": [],
   "source": [
    "df['TCGA_DESC'] = df['TCGA_DESC'].fillna(df['Cancer Type (matching TCGA label)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "id": "5nIgVWczpmyf",
    "outputId": "72745427-e636-4d64-e543-ba9e4a459354"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3f_ddXgvdPu"
   },
   "source": [
    "As you can see above, we filled practically all the missing values from Cancer Type (matching TCGA label) column and now they are identical to TCGA_DESC. Hence, I am going to drop the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zDF9dWIIwuTZ",
    "outputId": "3eb2596b-48fd-41a9-da1d-b1a78319c000"
   },
   "outputs": [],
   "source": [
    "df['Cancer Type (matching TCGA label)'].equals( df['TCGA_DESC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SpShdXdFwVpB"
   },
   "outputs": [],
   "source": [
    "df.drop('TCGA_DESC', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "hiVdSywoz6xi",
    "outputId": "64f3c894-8801-4d9a-a4f2-42bd617a97f1"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9g7tRr1KqsK"
   },
   "outputs": [],
   "source": [
    "df.replace('?', np.NaN, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "34R63RBYNNqR",
    "outputId": "609ddc90-479f-4e06-c639-bff4185d8c87"
   },
   "outputs": [],
   "source": [
    "df['Cancer Type (matching TCGA label)'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "juxxvj7LN-2h",
    "outputId": "62074aa7-5d3d-4994-c56b-9e2287051a13"
   },
   "outputs": [],
   "source": [
    "df.groupby(['Cancer Type (matching TCGA label)'])['GDSC Tissue descriptor 2'].agg([lambda x: list(x.unique()), 'nunique'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfyBSqGxSHEA"
   },
   "source": [
    "I will first replace the NaN values in 'GDSC Tissue descriptor 2' whith the most frequent value that corresponds to the 'Cancer Type (matching TCGA label)' value. Only for those values of Cancer Type that corresponf to multiple values of GDSC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ct-H0sRGSGZq",
    "outputId": "2dd77218-0326-4fbd-919a-9c99062196ea"
   },
   "outputs": [],
   "source": [
    "filtered_df_LAML = df[df['Cancer Type (matching TCGA label)'] == 'LAML']\n",
    "most_frequent_value_LAML = filtered_df_LAML['GDSC Tissue descriptor 2'].mode().iloc[0]\n",
    "print(\"Most frequent value corresponding to LAML :\", most_frequent_value_LAML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTm9XDM_Yldp"
   },
   "outputs": [],
   "source": [
    "df.loc[df['Cancer Type (matching TCGA label)'] == \"LAML\",'GDSC Tissue descriptor 2'] = \\\n",
    "df.loc[df['Cancer Type (matching TCGA label)'] == \"LAML\",'GDSC Tissue descriptor 2'].replace(np.NaN, most_frequent_value_LAML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3B_Wn6bQVg6H",
    "outputId": "177d0f4c-f2fe-4718-a1a6-3e85fd8b745e"
   },
   "outputs": [],
   "source": [
    "filtered_df_UNCLASSIFIED = df[df['Cancer Type (matching TCGA label)'] == 'UNCLASSIFIED']\n",
    "most_frequent_value_UNCLASSIFIED = filtered_df_UNCLASSIFIED['GDSC Tissue descriptor 2'].mode().iloc[0]\n",
    "print(\"Most frequent value corresponding to UNCLASSIFIED :\", most_frequent_value_UNCLASSIFIED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3uOicH2cykG"
   },
   "outputs": [],
   "source": [
    "df.loc[df['Cancer Type (matching TCGA label)'] == \"UNCLASSIFIED\", 'GDSC Tissue descriptor 2'] = \\\n",
    "df.loc[df['Cancer Type (matching TCGA label)'] == \"UNCLASSIFIED\", 'GDSC Tissue descriptor 2'].replace(np.NaN, most_frequent_value_UNCLASSIFIED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bD_YUp1dMDAg"
   },
   "source": [
    "I'll replace missing values from 'GDSC Tissue descriptor 2' with correspopnding value from the same column, based on the value that 'Cancer Type (matching TCGA label)' has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0w_edjKyHNVL"
   },
   "outputs": [],
   "source": [
    "df['GDSC Tissue descriptor 2'] = df.groupby('Cancer Type (matching TCGA label)')['GDSC Tissue descriptor 2'] \\\n",
    "                                   .transform(lambda x: x.ffill().bfill())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8zhsBCjMLLv_",
    "outputId": "ff4f9b4e-bac9-4480-a0ae-206ebbb95e72"
   },
   "outputs": [],
   "source": [
    "df.groupby(['Cancer Type (matching TCGA label)'])['GDSC Tissue descriptor 2'].agg([lambda x: list(x.unique()), 'nunique'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "id": "8iwuOB8iLbtA",
    "outputId": "abd5287a-d5a8-4415-8be4-c855c598ae7c"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_oreLk1o0seS",
    "outputId": "04955021-3c2f-4aa5-e46c-9b13ab249e32"
   },
   "outputs": [],
   "source": [
    "df.groupby(['GDSC Tissue descriptor 2'])['Cancer Type (matching TCGA label)'].agg([lambda x: list(x.unique()), 'nunique'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OKiGN2k1Na4M",
    "outputId": "04e17fe3-ecba-4dbc-a7b3-de4682fd0ccd"
   },
   "outputs": [],
   "source": [
    "df.groupby(['Cancer Type (matching TCGA label)'])['GDSC Tissue descriptor 1'].agg([lambda x: list(x.unique()), 'nunique'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qcc93QgNfc5O"
   },
   "source": [
    "I will first replace the NaN values in 'GDSC Tissue descriptor 1' whith the most frequent value that corresponds to the 'Cancer Type (matching TCGA label)' value. Only for those values of Cancer Type that corresponf to multiple values of GDSC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BH-aDleCfg-_",
    "outputId": "55e8558c-56cc-4919-8425-d15e942467db"
   },
   "outputs": [],
   "source": [
    "filtered_df_UNCLASSIFIED = df[df['Cancer Type (matching TCGA label)'] == 'UNCLASSIFIED']\n",
    "most_frequent_value_UNCLASSIFIED = filtered_df_UNCLASSIFIED['GDSC Tissue descriptor 1'].mode().iloc[0]\n",
    "print(\"Most frequent value corresponding to UNCLASSIFIED :\", most_frequent_value_UNCLASSIFIED)\n",
    "df.loc[df['Cancer Type (matching TCGA label)'] == 'UNCLASSIFIED', 'GDSC Tissue descriptor 1'] = \\\n",
    "df.loc[df['Cancer Type (matching TCGA label)'] == 'UNCLASSIFIED', 'GDSC Tissue descriptor 1'] \\\n",
    ".replace(np.NaN, most_frequent_value_UNCLASSIFIED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hrTWMHANdjH"
   },
   "source": [
    "I'll replace missing values from 'GDSC Tissue descriptor 1' with correspopnding value from the same column, based on the value that 'Cancer Type (matching TCGA label)' has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8f_C_cWOZMj"
   },
   "outputs": [],
   "source": [
    "df['GDSC Tissue descriptor 1'] = df.groupby('Cancer Type (matching TCGA label)')['GDSC Tissue descriptor 1'] \\\n",
    "                                   .transform(lambda x: x.ffill().bfill())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "id": "FbEllbm95rME",
    "outputId": "62165e68-b09d-46e1-f565-446204721d76"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnaNHjgcIZDe"
   },
   "source": [
    "Let's check the association between 'Microsatellite instability Status (MSI)' and 'Cancer Type (matching TCGA label)', as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "PvsFVNFcDtqi",
    "outputId": "42e2003f-7d20-4079-f107-7b6b67e5dfff"
   },
   "outputs": [],
   "source": [
    "df.groupby(\"Microsatellite instability Status (MSI)\")[\"Cancer Type (matching TCGA label)\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-1_koDFIrHc"
   },
   "source": [
    "We can't replace missing cancer type based on MSI because there isn't a 1-1 relationship (for each MSI we could use many possible cancer type values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eRNiYnzVEurQ",
    "outputId": "c71a4388-6439-4e85-a569-85ae6c66e963"
   },
   "outputs": [],
   "source": [
    "df.groupby(\"Cancer Type (matching TCGA label)\")[\"Microsatellite instability Status (MSI)\"].agg([lambda x: list(x.unique()), 'nunique'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K41jHmm_IoWR"
   },
   "source": [
    "However, we can replace missing values of 'MSI', according to the value of 'Cancer Type (matching TCGA label)'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwhwnILMGqEa"
   },
   "source": [
    "I will replace the NaN values in 'MSI' whith the most frequent value that corresponds to the 'Cancer Type (matching TCGA label)' value, for those values of Cancer Type that correspond to multiple values of 'MSI'. As for the NaN values in 'MSI' that correspond to values of Cancer Type that have single values of 'MSI', i will replace them with that single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zjT9hwxF7i4",
    "outputId": "47dbd54d-9f64-431c-be8d-a6947fd09972"
   },
   "outputs": [],
   "source": [
    "# Step 1: Calculate unique counts of MSI values per cancer type\n",
    "msi_unique_counts = df.groupby(\"Cancer Type (matching TCGA label)\")[\"Microsatellite instability Status (MSI)\"].nunique()\n",
    "# Step 2: Replace NaNs when there's only 1 unique non-NaN value\n",
    "single_value_types = msi_unique_counts[msi_unique_counts == 1].index  # Get cancer types with 1 unique MSI value\n",
    "for cancer_type in single_value_types:\n",
    "    unique_value = df.loc[df['Cancer Type (matching TCGA label)'] == cancer_type, 'Microsatellite instability Status (MSI)'].dropna().iloc[0]\n",
    "    df.loc[(df['Cancer Type (matching TCGA label)'] == cancer_type) & df['Microsatellite instability Status (MSI)'].isna(), 'Microsatellite instability Status (MSI)'] = unique_value\n",
    "# Step 3: Replace NaNs for multi-value cancer types using the most frequent value\n",
    "multi_value_types = msi_unique_counts[msi_unique_counts > 1].index  # Get cancer types with multiple MSI values\n",
    "for cancer_type in multi_value_types:\n",
    "    most_frequent_value = df.loc[df['Cancer Type (matching TCGA label)'] == cancer_type, 'Microsatellite instability Status (MSI)'].mode().iloc[0]\n",
    "    df.loc[(df['Cancer Type (matching TCGA label)'] == cancer_type) & df['Microsatellite instability Status (MSI)'].isna(), 'Microsatellite instability Status (MSI)'] = most_frequent_value\n",
    "print(\"NaN values successfully replaced!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "id": "sLT3qVBieWla",
    "outputId": "f3f6300f-85cd-4b58-f41e-46c79a504499"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hLeC90tT6mf"
   },
   "source": [
    "Let's check the association between 'Microsatellite instability Status (MSI)' and 'GDSC Tissue descriptor 1', as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "fqebYNPVSilP",
    "outputId": "4b2cc665-32ab-46b7-fd29-e13a6ccd9de8"
   },
   "outputs": [],
   "source": [
    "df.groupby(\"Microsatellite instability Status (MSI)\")[\"GDSC Tissue descriptor 1\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7ieybFHUzq1"
   },
   "source": [
    "Likewise, we can't replace missing 'GDSC Tissue descriptor 1' based on 'MSI' because there isn't a 1-1 relationship (for each MSI we could use many possible GDSC values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eC2Z2ONDoHMQ"
   },
   "source": [
    "Likewise, we can't replace missing 'GDSC Tissue descriptor 2' based on 'MSI' because there isn't a 1-1 relationship (for each MSI we could use many possible GDSC values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "QEVK4WMJj82M",
    "outputId": "f2ef5e3d-bcbd-460a-da44-a6b546843a94"
   },
   "outputs": [],
   "source": [
    "df.groupby(\"Microsatellite instability Status (MSI)\")[\"GDSC Tissue descriptor 2\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tV0EQe_XsEWe",
    "outputId": "eb4038fb-65dd-4124-bdca-9551a3cef26e"
   },
   "outputs": [],
   "source": [
    "df.groupby(\"GDSC Tissue descriptor 2\")[\"GDSC Tissue descriptor 1\"].agg([lambda x: list(x.unique()), 'nunique'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "aTcKQa36seJ5",
    "outputId": "eb79ab6e-fed4-4ae7-938e-fb83bf3df374"
   },
   "outputs": [],
   "source": [
    "df[df[\"GDSC Tissue descriptor 1\"].isna() & df[\"GDSC Tissue descriptor 2\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzgFsrErt7Ek"
   },
   "source": [
    "The two GDSC columns have common missing values, so I can't use them to treplace each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0qhGjDNgib9"
   },
   "source": [
    "Let's check Target and Target Pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "89PcUXw_2KZv",
    "outputId": "414a1f78-ac75-450a-d168-b5f3b0c58f76"
   },
   "outputs": [],
   "source": [
    "df.groupby(\"TARGET\", dropna=False)['TARGET_PATHWAY'].agg([lambda x: list(x.unique()), 'nunique'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 834
    },
    "id": "Ex-8gnIrhFXq",
    "outputId": "6f52f3c4-382c-4595-99fe-3160ccbde061"
   },
   "outputs": [],
   "source": [
    "df.groupby('TARGET_PATHWAY')[\"TARGET\"].agg([lambda x: list(x.unique()), 'nunique'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wzC2kF8kDdC"
   },
   "source": [
    "I will replace the NaN values in 'TARGET' whith the most frequent value that corresponds to the 'TARGET_PATHWAY' value, for those values of 'TARGET_PATHWAY' that correspond to multiple values of 'TARGET'. As for the NaN values in 'TARGET' that correspond to values of 'TARGET_PATHWAY' that have single values of 'TARGET', i will replace them with that single value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7pG_f69JjwgJ",
    "outputId": "a6c7fa72-5a2f-43a5-e95f-4eb30ea96ff5"
   },
   "outputs": [],
   "source": [
    "# Step 1: Calculate unique counts of TARGET values per TARGET_PATHWAY\n",
    "target_unique_counts = df.groupby(\"TARGET_PATHWAY\")[\"TARGET\"].nunique()\n",
    "# Step 2: Replace NaNs when there's only 1 unique non-NaN value\n",
    "single_value_types = target_unique_counts[target_unique_counts == 1].index  # Get TARGET_PATHWAY with 1 unique TARGET value\n",
    "for target_path in single_value_types:\n",
    "    unique_value = df.loc[df['TARGET_PATHWAY'] == target_path, 'TARGET'].dropna().iloc[0]\n",
    "    df.loc[(df['TARGET_PATHWAY'] == target_path) & df['TARGET'].isna(), 'TARGET'] = unique_value\n",
    "# Step 3: Replace NaNs for multi-value TARGET_PATHWAY using the most frequent value\n",
    "multi_value_types = target_unique_counts[target_unique_counts > 1].index  # Get TARGET_PATHWAY with multiple TARGET values\n",
    "for target_path in multi_value_types:\n",
    "    most_frequent_value = df.loc[df['TARGET_PATHWAY'] == target_path, 'TARGET'].mode().iloc[0]\n",
    "    df.loc[(df['TARGET_PATHWAY'] == target_path) & df['TARGET'].isna(), 'TARGET'] = most_frequent_value\n",
    "print(\"NaN values successfully replaced!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "id": "-Q2zV3odrFJi",
    "outputId": "ad57320f-965e-4b5f-f826-7647942a0cdf"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OY6p8WZORkk"
   },
   "source": [
    "The missing values in 'CNA', 'Screen Medium', 'Growth Properties', 'Gene Expression', 'Methylation' have the same value in the MSI column. Based on that value, I am going to find the most frequent value for this fatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "id": "nGbgm90hIOMh",
    "outputId": "ecacda97-d998-4de5-acf4-7dc2ba2dc269"
   },
   "outputs": [],
   "source": [
    "df[df['Growth Properties'].isna()]['Microsatellite instability Status (MSI)'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "Jn1kQvF9M1m7",
    "outputId": "e5a62a68-e353-4a04-c6aa-bf671a61b880"
   },
   "outputs": [],
   "source": [
    "df[df['Microsatellite instability Status (MSI)'] == 'MSS/MSI-L']['CNA'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zo7WxBRzhydG"
   },
   "outputs": [],
   "source": [
    "for feature in df[['CNA', 'Screen Medium', 'Growth Properties', 'Gene Expression', 'Methylation']]:\n",
    "    df[feature] = df[feature].fillna(df[df['Microsatellite instability Status (MSI)'] == 'MSS/MSI-L'][feature].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "id": "tuNRQO4chlLT",
    "outputId": "cdc71bf5-9bf8-4f8a-806f-a796a8f82066"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "V_nu9mUZjiOk",
    "outputId": "cf133d2a-8990-401c-b719-9c034ab36f7d"
   },
   "outputs": [],
   "source": [
    "df.groupby('Cancer Type (matching TCGA label)', dropna=False)[\"Z_SCORE\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "typn30cemdVK"
   },
   "source": [
    "Based on the above we see that the NaN values are closer to the NB cancer type. So I could replace the NaN values in cncer types with NB, which is a bit to average. Instead, I am going to use the the similarity of LN_IC50 for each cancer type, in a specific drug group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "VGNhje9rr-09",
    "outputId": "e5b61514-c8f5-4080-a401-a9392cdfea59"
   },
   "outputs": [],
   "source": [
    "sns.histplot(x=df['LN_IC50'], bins=30, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "oyo6qnTIz5TG",
    "outputId": "d791c5ee-ab9b-4044-a20f-bfa9916d882d"
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x=df['LN_IC50'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMASiqgG0G4E"
   },
   "source": [
    "Since we have tail and outliers, I am going to use median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FC_1GL713ZAR",
    "outputId": "65d19e80-965f-489a-aab0-e572840da84f"
   },
   "outputs": [],
   "source": [
    "columns_to_fill = ['Cancer Type (matching TCGA label)', 'GDSC Tissue descriptor 1', 'GDSC Tissue descriptor 2']\n",
    "\n",
    "for col in columns_to_fill:\n",
    "    median_ic50 = df.groupby(['DRUG_NAME', col])['LN_IC50'].median().reset_index()\n",
    "    df = df.merge(median_ic50, on=['DRUG_NAME', col], how='left', suffixes=('', '_median'))\n",
    "    df[col] = df.groupby('DRUG_NAME')[col].transform(lambda x: x.ffill().bfill())\n",
    "    df.drop('LN_IC50_median', axis=1, inplace=True)\n",
    "\n",
    "print(df[columns_to_fill].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "id": "nYWE_2eegKH-",
    "outputId": "71643012-cfeb-4e62-b4ee-25c4db696bf8"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "Kc1JC-49gPsV",
    "outputId": "ec4e68db-a00d-4b9c-90ce-38fcd4256a68"
   },
   "outputs": [],
   "source": [
    "df.groupby(\"DRUG_ID\")[\"DRUG_NAME\"].agg([lambda x: x.unique(), 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGDOiRjloXUt"
   },
   "source": [
    "I am going to drop drug_name and cosmic_name since they're redundant and keep drug_id and cosmic_id which is more suitable for tree based models. However, I am going to transform them to category just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b94vPRGOprEP"
   },
   "outputs": [],
   "source": [
    "df['DRUG_ID'] = df['DRUG_ID'].astype('category')\n",
    "df['COSMIC_ID'] = df['COSMIC_ID'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUc9xPE06za1"
   },
   "source": [
    "<h4>Note for later: Here I start for Linear regression, with <b>df</b> dataframe</4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3w1FgN8ism3e"
   },
   "outputs": [],
   "source": [
    "df_id = df.drop(['DRUG_NAME', 'CELL_LINE_NAME'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "_i2xujoBswXi",
    "outputId": "697cc41d-94b6-469b-d12a-d27dda51796b"
   },
   "outputs": [],
   "source": [
    "df_id.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8G3ngDSTzx8E"
   },
   "source": [
    "<h3><b>3.2</b> Data Encoding</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNOZxurqyZAh"
   },
   "source": [
    "<b>3.2.1</b> I'll transform features with 2 unique values to binary features  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xqRf444ylSe"
   },
   "outputs": [],
   "source": [
    "binary_features = [col for col in df_id.columns if df_id[col].nunique() == 2]\n",
    "for feature in binary_features:\n",
    "    df_id[feature] = (df_id[feature] == df_id[feature].unique()[0]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W56AjgK82zfA"
   },
   "source": [
    "<b>3.2.2</b> I'll transform features with 3 unique values with one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MS0GtYHj5tIc"
   },
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False, drop=None, handle_unknown='ignore')\n",
    "encoded_features = encoder.fit_transform(df_id[['Growth Properties']])\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(['Growth Properties']))\n",
    "df_id = pd.concat([df_id.drop('Growth Properties', axis=1), encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "216kdst1y2aR",
    "outputId": "70d669bb-7b65-447d-b857-96123b8f7163"
   },
   "outputs": [],
   "source": [
    "df_id.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqMinES8QDlf"
   },
   "source": [
    "Let's check for linear relationship between the ID's and the target, to choose encoding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "CuYWCBQ3Pmok",
    "outputId": "c8627481-f85b-448e-e29e-423bf7aed4e9"
   },
   "outputs": [],
   "source": [
    "df_id[['DRUG_ID', 'COSMIC_ID', 'LN_IC50']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myPE4-V-QPNf"
   },
   "source": [
    "No, linear relationship. Let's check ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UjGrCdd0Ql-C",
    "outputId": "bcf184f3-4d85-4358-ac35-495969bdf17b"
   },
   "outputs": [],
   "source": [
    "# Filter only non-null values for COSMIC_ID\n",
    "df_no_nan = df_id.dropna(subset=['COSMIC_ID', 'DRUG_ID'])\n",
    "\n",
    "# Group `LN_IC50` by `COSMIC_ID`\n",
    "categories_cosmic = df_no_nan['COSMIC_ID'].unique()\n",
    "ln_ic50_groups_cosmic = [df_no_nan[df_no_nan['COSMIC_ID'] == cat]['LN_IC50'] for cat in categories_cosmic]\n",
    "\n",
    "# Perform ANOVA for COSMIC_ID\n",
    "anova_result_cosmic = stats.f_oneway(*ln_ic50_groups_cosmic)\n",
    "print(f\"ANOVA p-value for COSMIC_ID: {anova_result_cosmic.pvalue}\")\n",
    "\n",
    "# Group `LN_IC50` by `DRUG_ID`\n",
    "categories_drug = df_no_nan['DRUG_ID'].unique()\n",
    "ln_ic50_groups_drug = [df_no_nan[df_no_nan['DRUG_ID'] == cat]['LN_IC50'] for cat in categories_drug]\n",
    "\n",
    "# Perform ANOVA for DRUG_ID\n",
    "anova_result_drug = stats.f_oneway(*ln_ic50_groups_drug)\n",
    "print(f\"ANOVA p-value for DRUG_ID: {anova_result_drug.pvalue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pAz2um5LTvPn",
    "outputId": "005f41a3-7977-4aa2-cacd-80eef61d8066"
   },
   "outputs": [],
   "source": [
    "print(categories_cosmic)\n",
    "print(categories_drug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-G3kfIGyOmzM"
   },
   "source": [
    "<b>3.2.3</b> Too many categories. Thus, I will use Label encoding for 'COSMIC_ID' ,\t'DRUG_ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xHmH3U9OsRS"
   },
   "outputs": [],
   "source": [
    "df_encoded = df_id.copy() # = ['DRUG_ID', 'COSMIC_ID']\n",
    "le_drug = LabelEncoder()\n",
    "le_cosmic = LabelEncoder()\n",
    "df_encoded['DRUG_ID'] = le_drug.fit_transform(df_encoded['DRUG_ID'])\n",
    "df_encoded['COSMIC_ID'] = le_cosmic.fit_transform(df_encoded['COSMIC_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKltvw_MBGi_"
   },
   "source": [
    "<b>3.2.4</b> I'll use target encoding for the rest (high-cardinal) features, since we have a big data set and I am going to use tree based regression, along with KFold to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8zTgC1aCF9ui",
    "outputId": "4fea9b05-2fe4-4e13-d140-b9b8a53b8e65"
   },
   "outputs": [],
   "source": [
    "categorical_features = ['GDSC Tissue descriptor 1',\n",
    "                        'GDSC Tissue descriptor 2', 'Cancer Type (matching TCGA label)',\n",
    "                        'TARGET', 'TARGET_PATHWAY']\n",
    "\n",
    "# Define the target variable\n",
    "target = 'LN_IC50'\n",
    "# Step 1: Split the dataset into train & test **before encoding**\n",
    "train_df, test_df = train_test_split(df_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Number of folds\n",
    "K = 5\n",
    "kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "# Make copies to avoid modifying the original dataset\n",
    "train_encoded = train_df.copy()\n",
    "test_encoded = test_df.copy()\n",
    "\n",
    "# Initialize dictionary to store target means\n",
    "means_dict = {}\n",
    "\n",
    "# Apply K-Fold Target Encoding on the training set\n",
    "for col in categorical_features:\n",
    "    train_encoded[col + '_encoded'] = np.nan  # Create new column for encoding\n",
    "\n",
    "    for train_idx, val_idx in kf.split(train_encoded):\n",
    "        train_fold = train_encoded.iloc[train_idx]\n",
    "        val_fold = train_encoded.iloc[val_idx]\n",
    "\n",
    "        # Compute mean target for each category (excluding current fold)\n",
    "        fold_means = train_fold.groupby(col)[target].mean()\n",
    "\n",
    "        # Apply encoding to validation fold\n",
    "        train_encoded.iloc[val_idx, train_encoded.columns.get_loc(col + '_encoded')] = val_fold[col].map(fold_means)\n",
    "\n",
    "    # Store the global mean for test-time replacement\n",
    "    means_dict[col] = train_encoded.groupby(col)[target].mean()\n",
    "\n",
    "# Step 3: Apply Target Encoding to the test set using learned means from training\n",
    "for col in categorical_features:\n",
    "    test_encoded[col + '_encoded'] = test_encoded[col].map(means_dict[col])  # Apply learned means\n",
    "    test_encoded[col + '_encoded'] = test_encoded[col + '_encoded'].fillna(train_encoded[col + '_encoded'].mean())  # Fill unseen categories with global mean\n",
    "\n",
    "# Drop original categorical columns\n",
    "train_encoded = train_encoded.drop(columns=categorical_features)\n",
    "test_encoded = test_encoded.drop(columns=categorical_features)\n",
    "\n",
    "# Check results\n",
    "print(train_encoded.head())\n",
    "print(test_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFgjw64jbKgG"
   },
   "source": [
    "<h2><b>4.</b> Data Visualisation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Egbyw7EAc17D"
   },
   "source": [
    "<h3><b>4.1</b> Numerical</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dMtpD3hsa7d2",
    "outputId": "34c03c1f-5bb3-41bf-ed04-75a8e2178d15"
   },
   "outputs": [],
   "source": [
    "for feat in ['LN_IC50' , 'AUC', 'Z_SCORE']:\n",
    "  plt.figure(figsize=(8,6))\n",
    "  sns.histplot(x=train_encoded[feat], bins=30, kde=True)\n",
    "  plt.xlabel(feat)\n",
    "  plt.ylabel('Frequency')\n",
    "  plt.title(f'Distribution of {feat}')\n",
    "  plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "blrgqbXfmkHH",
    "outputId": "76660a49-3b8d-46cb-9a83-6f223e301990"
   },
   "outputs": [],
   "source": [
    "for feat in ['AUC', 'Z_SCORE', 'TARGET_encoded', 'TARGET_PATHWAY_encoded']:\n",
    "  plt.figure(figsize=(8,6))\n",
    "  sns.regplot(x=feat, y='LN_IC50', line_kws={'color':'red'}, data=train_encoded)\n",
    "  plt.xlabel(feat)\n",
    "  plt.ylabel('LN_IC50')\n",
    "  plt.title(f'LN_IC50 vs {feat}')\n",
    "  plt.show()\n",
    "  pearson_coef, p_value = stats.pearsonr(train_encoded[feat], train_encoded['LN_IC50'])\n",
    "  print(f\"The Pearson Correlation Coefficient of {feat} with LN_IC50 is: {pearson_coef}, with a P-value of: P = {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>4.2</b> Categorical</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4-h9iuttcdoK",
    "outputId": "b51bf74d-0cce-40c8-aa92-bd31bd73a81d"
   },
   "outputs": [],
   "source": [
    "for feat in ['Microsatellite instability Status (MSI)', 'Screen Medium',  'CNA', 'Gene Expression', \\\n",
    "             'Methylation', 'Growth Properties_Adherent', 'Growth Properties_Semi-Adherent', \\\n",
    "             'Growth Properties_Suspension']:\n",
    "  plt.figure(figsize=(8,6))\n",
    "  sns.boxplot(x=feat, y='LN_IC50', data=train_encoded)\n",
    "  plt.xlabel(feat)\n",
    "  plt.ylabel('LN_IC50')\n",
    "  plt.title(f'Boxplot of {feat} vs LN_IC50')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STFx8gE1fMWX"
   },
   "source": [
    "As we can see many outliers and great overlap, so linear regression isn't the best choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5x6RM9RgeRbc",
    "outputId": "254ffaae-c43e-4f7d-ddda-f65978220442"
   },
   "outputs": [],
   "source": [
    "for feat in ['GDSC Tissue descriptor 1',  'GDSC Tissue descriptor 2', \\\n",
    "             'Cancer Type (matching TCGA label)', 'TARGET', 'TARGET_PATHWAY']:\n",
    "  plt.figure(figsize=(16,9))\n",
    "  sns.barplot(x='LN_IC50', y=feat , orient='h', data=df)\n",
    "  plt.xlabel('LN_IC50')\n",
    "  plt.ylabel(feat)\n",
    "  plt.title(f'Barplot of {feat} vs LN_IC50')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "id": "ApEX9QB4hZZi",
    "outputId": "7e8025e3-58c3-44fc-ffc6-b5baf414e308"
   },
   "outputs": [],
   "source": [
    "cancer_types = df.groupby('Cancer Type (matching TCGA label)')['LN_IC50'].mean().sort_values(ascending=False)\n",
    "plt.figure(figsize=(16,9))\n",
    "sns.barplot(x=cancer_types.index, y=cancer_types.values)\n",
    "plt.xlabel('Cancer Type')\n",
    "plt.ylabel('Mean LN_IC50')\n",
    "plt.title('Mean LN_IC50 by Cancer Type')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "id": "l5hSN9hyqkkB",
    "outputId": "05e540c1-476c-495b-d75b-879dffaf48c7"
   },
   "outputs": [],
   "source": [
    "tissue_types = df.groupby('GDSC Tissue descriptor 1')['AUC'].median().sort_values(ascending=False)\n",
    "plt.figure(figsize=(16,9))\n",
    "sns.boxplot(x=tissue_types.index, y=tissue_types.values)\n",
    "plt.xlabel('GDSC Tissue descriptor 1')\n",
    "plt.ylabel('Median AUC')\n",
    "plt.title('Median AUC by GDSC Tissue descriptor 1')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525
    },
    "id": "XvhV7tKOtLDT",
    "outputId": "e922b6d4-0f9d-49eb-c597-689737819545"
   },
   "outputs": [],
   "source": [
    "top_targets = df['TARGET'].value_counts().nlargest(10)\n",
    "plt.figure(figsize=(12,7))\n",
    "sns.barplot(x=top_targets.values, y=top_targets.index,\\\n",
    "            orient='h', hue=top_targets.index)\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Top 10 Targets by Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "lbJFXx6ZwPEt",
    "outputId": "c97c85a5-1cdf-4eac-ee34-bda14c87f859"
   },
   "outputs": [],
   "source": [
    "pathway_counts = df['TARGET_PATHWAY'].value_counts()\n",
    "plt.figure(figsize=(12,7))\n",
    "sns.barplot(y=pathway_counts.index, x=pathway_counts.values, orient='h', hue=pathway_counts.index)\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Target Pathway')\n",
    "plt.title('Count of Target Pathways')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 849
    },
    "id": "7xvIPOGrwCkr",
    "outputId": "a3fecba4-f4ae-4314-84b5-c10719fb574e"
   },
   "outputs": [],
   "source": [
    "pivot_table = pd.pivot_table(df, values='LN_IC50', index='Cancer Type (matching TCGA label)', columns='DRUG_NAME', aggfunc='mean')\n",
    "plt.figure(figsize=(20,16))\n",
    "sns.heatmap(pivot_table, cmap='coolwarm', annot=False)\n",
    "plt.xlabel('DRUG_NAME')\n",
    "plt.ylabel('Cancer Type (matching TCGA label)')\n",
    "plt.title('Heatmap of LN_IC50 across Cancer types and Drug Name')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGRaY_DOuvry"
   },
   "source": [
    "Drugs effectiveness comparison for each tissue and specific condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "nZqJI42P2CbZ",
    "outputId": "d1a71e74-c51a-4677-ca46-eb06187114b7"
   },
   "outputs": [],
   "source": [
    "df_pivot = df.pivot_table(index=['GDSC Tissue descriptor 1', 'GDSC Tissue descriptor 2', 'Cancer Type (matching TCGA label)'], columns='DRUG_NAME', values='LN_IC50', aggfunc='median')\n",
    "plt.figure(figsize=(20,12))\n",
    "sns.heatmap(df_pivot, cmap='coolwarm', annot=False)\n",
    "plt.title('Median LN_IC50 by GDSC Tissue descriptor 1, GDSC Tissue descriptor 2, and Cancer Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBisHt5gvLnT"
   },
   "source": [
    "But that is too crowded. Lets's selcet the 10 most effective in each cancer type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "A0dX0S_WwN3I",
    "outputId": "fc015c9f-0269-4a28-fb35-59338f41ed76"
   },
   "outputs": [],
   "source": [
    "top_n = 10\n",
    "top_drugs_per_cancer = (\n",
    "df.groupby(['GDSC Tissue descriptor 1', 'GDSC Tissue descriptor 2',\\\n",
    "                                    'Cancer Type (matching TCGA label)',\\\n",
    "                                    'DRUG_NAME'])['LN_IC50'].median().reset_index().sort_values(['GDSC Tissue descriptor 1', 'GDSC Tissue descriptor 2',\\\n",
    "                                    'Cancer Type (matching TCGA label)', 'Cancer Type (matching TCGA label)',\\\n",
    "                                    'LN_IC50']).groupby(['GDSC Tissue descriptor 1', 'GDSC Tissue descriptor 2',\\\n",
    "                                    'Cancer Type (matching TCGA label)','Cancer Type (matching TCGA label)']).head(top_n)\n",
    ")\n",
    "#print(top_drugs_per_cancer)\n",
    "df_pivot = top_drugs_per_cancer.pivot_table(index=['GDSC Tissue descriptor 1', 'GDSC Tissue descriptor 2', 'Cancer Type (matching TCGA label)'], columns='DRUG_NAME', values='LN_IC50', aggfunc='median')\n",
    "#print(df_pivot)\n",
    "plt.figure(figsize=(20,12))\n",
    "sns.heatmap(df_pivot, cmap='coolwarm', annot=False)\n",
    "plt.title('Top 10 Most Effective Drugs per Cancer Type')\n",
    "plt.xlabel('Drug Name')\n",
    "plt.ylabel('Cancer Type / Tissue Descriptor')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrb9t9Tczf4t"
   },
   "source": [
    "<h2><b>5.</b> Data Analysis</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>5.1</b> Correlation </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 863
    },
    "id": "6z5bfHRmySYR",
    "outputId": "817e53ef-e660-4718-82aa-a2ae8c9afc59"
   },
   "outputs": [],
   "source": [
    "corr_matrix = train_encoded.corr()\n",
    "plt.figure(figsize=(14,12))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnHgZYOHAmGN"
   },
   "source": [
    "<h3><b>5.2</b> Find Outliers</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yveXEbrszL6d"
   },
   "source": [
    "<h4><b>5.2.1</b> Based on all drugs</h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ejdUWZ6V2QOz",
    "outputId": "04574a96-a13e-49c7-fb94-e6375116b269"
   },
   "outputs": [],
   "source": [
    "def find_outliers_by_drug(df, numeric_cols=['LN_IC50', 'AUC', 'Z_SCORE']):\n",
    "    outliers = {}\n",
    "\n",
    "    for drug in df['DRUG_ID'].unique():\n",
    "        drug_data = df[df['DRUG_ID'] == drug]\n",
    "        drug_outliers = {}\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            v = drug_data[col]\n",
    "            q1 = v.quantile(0.25)\n",
    "            q3 = v.quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            outliers_count = ((v < lower_bound) | (v > upper_bound)).sum()\n",
    "            perc = outliers_count * 100.0 / len(drug_data)\n",
    "            drug_outliers[col] = (perc, outliers_count)\n",
    "            print(f\"Drug: {drug}, Column: {col} outliers = {perc:.2f}% ({outliers_count} out of {len(drug_data)})\")\n",
    "\n",
    "        outliers[drug] = drug_outliers\n",
    "\n",
    "    return outliers\n",
    "\n",
    "# Find outliers in the DataFrame for each drug\n",
    "outliers = find_outliers_by_drug(train_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVT7YUE1zTDE"
   },
   "source": [
    "<h4><b>5.2.2</b> Based on Cancer type</h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8Yg6Y1GtWT0",
    "outputId": "c23fb8d7-0dbe-49b7-8e05-4e8c1a9d5bc2"
   },
   "outputs": [],
   "source": [
    "def find_outliers_by_cancer(df, numeric_cols=['LN_IC50', 'AUC', 'Z_SCORE']):\n",
    "    outliers = {}\n",
    "\n",
    "    for cancer in df['Cancer Type (matching TCGA label)'].unique():\n",
    "        cancer_data = df[df['Cancer Type (matching TCGA label)'] == cancer]\n",
    "        cancer_outliers = {}\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            v = cancer_data[col]\n",
    "            q1 = v.quantile(0.25)\n",
    "            q3 = v.quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            outliers_count = ((v < lower_bound) | (v > upper_bound)).sum()\n",
    "            perc = outliers_count * 100.0 / len(cancer_data)\n",
    "            cancer_outliers[col] = (perc, outliers_count)\n",
    "            print(f\"Cancer: {cancer}, Column: {col} outliers = {perc:.2f}% ({outliers_count} out of {len(cancer_data)})\")\n",
    "\n",
    "        outliers[cancer] = cancer_outliers\n",
    "\n",
    "    return outliers\n",
    "\n",
    "# Find outliers in the DataFrame for each cancer\n",
    "outliers = find_outliers_by_cancer(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtN45YHMzfUE"
   },
   "source": [
    "<h4><b>5.2.3</b> Based on drugs used in each medical condition</h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uwVHDROkvAcv"
   },
   "outputs": [],
   "source": [
    "def find_outliers_by_medical_condition(df, group_cols, numeric_cols):\n",
    "\n",
    "    outliers = {}\n",
    "\n",
    "    # Group by medical condition (three categorical columns)\n",
    "    grouped = df.groupby(group_cols)\n",
    "\n",
    "    for group, subset in grouped:\n",
    "        condition_key = \"_\".join(map(str, group))  # Unique key for each group\n",
    "        outliers[condition_key] = {}\n",
    "        unique_drugs = subset['DRUG_NAME'].unique()  # Get unique drugs in this condition\n",
    "\n",
    "        for drug in unique_drugs:\n",
    "            drug_data = subset[subset['DRUG_NAME'] == drug]\n",
    "            drug_outliers = {}\n",
    "\n",
    "            for col in numeric_cols:\n",
    "                v = drug_data[col]\n",
    "                q1 = v.quantile(0.25)\n",
    "                q3 = v.quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                lower_bound = q1 - 1.5 * iqr\n",
    "                upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "                outliers_count = ((v < lower_bound) | (v > upper_bound)).sum()\n",
    "                perc = outliers_count * 100.0 / len(drug_data)\n",
    "\n",
    "                drug_outliers[col] = (perc, outliers_count)\n",
    "                print(f\"Medical Condition: {condition_key}, Drug: {drug}, Column: {col} → Outliers: {perc:.2f}% ({outliers_count})\")\n",
    "\n",
    "            outliers[condition_key][drug] = drug_outliers\n",
    "\n",
    "    return outliers\n",
    "\n",
    "# Columns to group by (medical condition) and numerical columns for outlier detection\n",
    "group_columns = ['GDSC Tissue descriptor 1', 'GDSC Tissue descriptor 2', 'Cancer Type (matching TCGA label)']\n",
    "numeric_columns = ['LN_IC50', 'AUC', 'Z_SCORE']\n",
    "\n",
    "# Run the function\n",
    "outliers_by_condition = find_outliers_by_medical_condition(df, group_columns, numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PDZkmb2wK92"
   },
   "source": [
    "From the three previous methods of calculating outliers, I prefer the last one because it is more specific for the drugs per condition and thus it preserves more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k58oG4s-087O"
   },
   "source": [
    "<h3><b>5.3</b> Calculate skewness</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>5.3.1</b> Based on all drugs</h4> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "x9qjc8wy0Xo-",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "1abd7da8-0cb1-4435-f48e-861879bc7aae",
    "scrolled": true
   },
   "source": [
    "def check_outliers_and_skewness(df, numeric_cols):\n",
    "    skewness_info = {}\n",
    "    outlier_info = {}\n",
    "\n",
    "    for drug in df['DRUG_NAME'].unique()[:2]:  # Only first two drugs\n",
    "        drug_data = df[df['DRUG_NAME'] == drug]\n",
    "        skewness_info[drug] = {}\n",
    "        outlier_info[drug] = {}\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            # Calculate skewness\n",
    "            col_skewness = skew(drug_data[col].dropna())\n",
    "            skewness_info[drug][col] = col_skewness\n",
    "\n",
    "            # Identify outliers using IQR method\n",
    "            Q1 = drug_data[col].quantile(0.25)\n",
    "            Q3 = drug_data[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            outliers = drug_data[(drug_data[col] < lower_bound) | (drug_data[col] > upper_bound)][col]\n",
    "            outlier_info[drug][col] = outliers\n",
    "\n",
    "            # Plot distribution and box plot\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            fig.suptitle(f'Distribution and Box Plot of {col} for {drug}')\n",
    "\n",
    "            # Histogram\n",
    "            sns.histplot(drug_data[col], kde=True, ax=ax1)\n",
    "            ax1.axvline(lower_bound, color='r', linestyle='--', label='IQR bounds')\n",
    "            ax1.axvline(upper_bound, color='r', linestyle='--')\n",
    "            ax1.set_title(f'Histogram (Skewness: {col_skewness:.2f})')\n",
    "            ax1.legend()\n",
    "\n",
    "            # Box plot\n",
    "            sns.boxplot(x=drug_data[col], ax=ax2)\n",
    "            ax2.set_title('Box Plot')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            print(f\"Drug: {drug}, Column: {col}\")\n",
    "            print(f\"Skewness: {col_skewness:.2f}\")\n",
    "            print(f\"Number of outliers: {len(outliers)}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    return skewness_info, outlier_info\n",
    "\n",
    "\n",
    "numeric_cols = ['LN_IC50', 'AUC', 'Z_SCORE']\n",
    "skewness_info, outlier_info = check_outliers_and_skewness(df, numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>5.3.2</b> Based on Cancer type</h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qF8vQjE_1rf-",
    "outputId": "891bf445-8deb-4ef1-d78a-67ee7194818d"
   },
   "outputs": [],
   "source": [
    "def check_outliers_and_skewness_cancer(df, numeric_cols):\n",
    "    skewness_info = {}\n",
    "    outlier_info = {}\n",
    "\n",
    "    for cancer in df['Cancer Type (matching TCGA label)'].unique()[:2]:  # Only first two drugs\n",
    "        cancer_data = df[df['Cancer Type (matching TCGA label)'] == cancer]\n",
    "        skewness_info[cancer] = {}\n",
    "        outlier_info[cancer] = {}\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            # Calculate skewness\n",
    "            col_skewness = skew(cancer_data[col].dropna())\n",
    "            skewness_info[cancer][col] = col_skewness\n",
    "\n",
    "            # Identify outliers using IQR method\n",
    "            Q1 = cancer_data[col].quantile(0.25)\n",
    "            Q3 = cancer_data[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            outliers = cancer_data[(cancer_data[col] < lower_bound) | (cancer_data[col] > upper_bound)][col]\n",
    "            outlier_info[cancer][col] = outliers\n",
    "\n",
    "            # Plot distribution and box plot\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            fig.suptitle(f'Distribution and Box Plot of {col} for {cancer}')\n",
    "\n",
    "            # Histogram\n",
    "            sns.histplot(cancer_data[col], kde=True, ax=ax1)\n",
    "            ax1.axvline(lower_bound, color='r', linestyle='--', label='IQR bounds')\n",
    "            ax1.axvline(upper_bound, color='r', linestyle='--')\n",
    "            ax1.set_title(f'Histogram (Skewness: {col_skewness:.2f})')\n",
    "            ax1.legend()\n",
    "\n",
    "            # Box plot\n",
    "            sns.boxplot(x=cancer_data[col], ax=ax2)\n",
    "            ax2.set_title('Box Plot')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            print(f\"Cancer: {cancer}, Column: {col}\")\n",
    "            print(f\"Skewness: {col_skewness:.2f}\")\n",
    "            print(f\"Number of outliers: {len(outliers)}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    return skewness_info, outlier_info\n",
    "\n",
    "\n",
    "numeric_cols = ['LN_IC50', 'AUC', 'Z_SCORE']\n",
    "skewness_info, outlier_info = check_outliers_and_skewness_cancer(df, numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>5.3.3</b> Based on drugs used in each medical condition</h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3YARj9FH2RC4",
    "outputId": "ed2c83e3-cf6e-4229-a3c6-e881a6ac2b90"
   },
   "outputs": [],
   "source": [
    "def check_outliers_and_skewness_by_condition(df, group_cols, numeric_cols):\n",
    "\n",
    "    skewness_info = {}\n",
    "\n",
    "    outlier_info = {}\n",
    "\n",
    "\n",
    "    grouped = df.groupby(group_cols)\n",
    "\n",
    "\n",
    "    for group, subset in grouped:\n",
    "\n",
    "        condition_key = \"_\".join(map(str, group))  # Unique key for each condition\n",
    "\n",
    "        skewness_info[condition_key] = {}\n",
    "\n",
    "        outlier_info[condition_key] = {}\n",
    "\n",
    "\n",
    "        # Select first two drugs in this condition\n",
    "\n",
    "        unique_drugs = subset['DRUG_NAME'].unique()[:2]\n",
    "\n",
    "\n",
    "        for drug in unique_drugs:\n",
    "\n",
    "            drug_data = subset[subset['DRUG_NAME'] == drug]\n",
    "\n",
    "            skewness_info[condition_key][drug] = {}\n",
    "\n",
    "            outlier_info[condition_key][drug] = {}\n",
    "\n",
    "\n",
    "            for col in numeric_cols:\n",
    "\n",
    "                # Calculate skewness\n",
    "\n",
    "                col_skewness = skew(drug_data[col].dropna())\n",
    "\n",
    "                skewness_info[condition_key][drug][col] = col_skewness\n",
    "\n",
    "\n",
    "                # Identify outliers using IQR method\n",
    "\n",
    "                Q1 = drug_data[col].quantile(0.25)\n",
    "\n",
    "                Q3 = drug_data[col].quantile(0.75)\n",
    "\n",
    "                IQR = Q3 - Q1\n",
    "\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "                outliers = drug_data[(drug_data[col] < lower_bound) | (drug_data[col] > upper_bound)][col]\n",
    "\n",
    "                outlier_info[condition_key][drug][col] = outliers\n",
    "\n",
    "\n",
    "                # Plot distribution and box plot\n",
    "\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "                fig.suptitle(f'Distribution and Box Plot of {col} for {drug} (Condition: {condition_key})')\n",
    "\n",
    "\n",
    "                # Histogram\n",
    "\n",
    "                sns.histplot(drug_data[col], kde=True, ax=ax1)\n",
    "\n",
    "                ax1.axvline(lower_bound, color='r', linestyle='--', label='IQR bounds')\n",
    "\n",
    "                ax1.axvline(upper_bound, color='r', linestyle='--')\n",
    "\n",
    "                ax1.set_title(f'Histogram (Skewness: {col_skewness:.2f})')\n",
    "\n",
    "                ax1.legend()\n",
    "\n",
    "\n",
    "                # Box plot\n",
    "\n",
    "                sns.boxplot(x=drug_data[col], ax=ax2)\n",
    "\n",
    "                ax2.set_title('Box Plot')\n",
    "\n",
    "\n",
    "                plt.tight_layout()\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "                print(f\"Condition: {condition_key}, Drug: {drug}, Column: {col}\")\n",
    "\n",
    "                print(f\"Skewness: {col_skewness:.2f}\")\n",
    "\n",
    "                print(f\"Number of outliers: {len(outliers)}\")\n",
    "\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "\n",
    "    return skewness_info, outlier_info\n",
    "\n",
    "\n",
    "\n",
    "# Columns to group by (medical condition) and numeric columns for analysis\n",
    "\n",
    "group_columns = ['GDSC Tissue descriptor 1', 'GDSC Tissue descriptor 2', 'Cancer Type (matching TCGA label)']\n",
    "\n",
    "numeric_columns = ['LN_IC50', 'AUC', 'Z_SCORE']\n",
    "\n",
    "\n",
    "# Run the function\n",
    "\n",
    "skewness_info, outlier_info = check_outliers_and_skewness_by_condition(df, group_columns, numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4of-0Hn40tW"
   },
   "source": [
    "As we can see again the method that checks the outliers and skewness of dtugs grouped by condition is better (less outliers/more info/better explicability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GriJlZvo8Rti"
   },
   "source": [
    "<h3><b>5.4</b> Handle outliers and skewnness</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers_and_skewness(df, numeric_cols, yeojohnson_lambdas=None, fit_lambda=True):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Handles outliers and skewness for numeric columns while grouping by encoded tissue/cancer type columns.\n",
    "\n",
    "    - Uses Winsorization to cap extreme values.\n",
    "\n",
    "    - Applies Z-score normalization.\n",
    "\n",
    "    - Fits Yeo-Johnson transformation on train and applies same λ to test (avoids data leakage).\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "        df (pd.DataFrame): The dataset to process.\n",
    "\n",
    "        numeric_cols (list): List of numeric columns to process.\n",
    "\n",
    "        yeojohnson_lambdas (dict, optional): Dictionary storing λ values for test application.\n",
    "\n",
    "        fit_lambda (bool): Whether to compute λ (True for train, False for test).\n",
    "\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        df_processed (pd.DataFrame): The processed dataset.\n",
    "\n",
    "        yeojohnson_lambdas (dict, optional): The λ values for reuse (if training).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()  # Avoid modifying original data\n",
    "    yeojohnson_lambdas = {} if fit_lambda else yeojohnson_lambdas  # Store λ values only when fitting\n",
    "\n",
    "    # Group by descriptors and process per drug\n",
    "    grouped = df.groupby([\"GDSC Tissue descriptor 1_encoded\", \"GDSC Tissue descriptor 2_encoded\", \"Cancer Type (matching TCGA label)_encoded\"])\n",
    "\n",
    "    for (desc1, desc2, cancer_type), group in grouped:\n",
    "        for drug in group[\"DRUG_ID\"].unique():\n",
    "            drug_data = group[group[\"DRUG_ID\"] == drug].copy()\n",
    "\n",
    "            for col in numeric_cols:\n",
    "                values = drug_data[col].dropna()\n",
    "\n",
    "                if values.nunique() < 5:  # Skip if too few unique values\n",
    "                    continue\n",
    "\n",
    "                # Winsorization: Define specific limits per column\n",
    "                winsorization_limits = {\n",
    "                    \"LN_IC50\": (0.01, 0.01),\n",
    "                    \"AUC\": (0.05, 0.05),\n",
    "                    \"Z_SCORE\": (0.01, 0.01)\n",
    "                }\n",
    "                # Apply column-specific Winsorization:\n",
    "                lower, upper = winsorization_limits.get(col, (0.05, 0.05))\n",
    "                capped = winsorize(values, (lower, upper))\n",
    "\n",
    "                drug_data.loc[values.index, col] = pd.Series(capped, index=values.index)\n",
    "\n",
    "                # Apply Z-score normalization\n",
    "                drug_data[col] = zscore(drug_data[col])\n",
    "\n",
    "                # Recalculate skewness after winsorization\n",
    "                new_skewness = skew(drug_data[col].dropna())\n",
    "\n",
    "                # Apply Yeo-Johnson only if data is highly skewed\n",
    "                if abs(new_skewness) > 1:\n",
    "                    if fit_lambda:\n",
    "                        # Fit Yeo-Johnson on train and store λ\n",
    "                        transformed, lambda_ = yeojohnson(drug_data[col].dropna())\n",
    "                        yeojohnson_lambdas[(desc1, desc2, cancer_type, drug, col)] = lambda_\n",
    "                    else:\n",
    "                        # Apply the precomputed λ from training\n",
    "                        lambda_ = yeojohnson_lambdas.get((desc1, desc2, cancer_type, drug, col), None)\n",
    "                        if lambda_ is None:\n",
    "                            continue  # Skip if λ is missing (unseen data)\n",
    "\n",
    "                        transformed = yeojohnson(drug_data[col].dropna(), lmbda=lambda_)\n",
    "                    drug_data.loc[drug_data[col].dropna().index, col] = transformed\n",
    "\n",
    "            # Update the original dataframe\n",
    "            df.loc[drug_data.index, numeric_cols] = drug_data[numeric_cols]\n",
    "\n",
    "    return (df, yeojohnson_lambdas) if fit_lambda else df\n",
    "\n",
    "\n",
    "#  Step 1: Process Train and Store λ\n",
    "numeric_cols = [\"AUC\", \"Z_SCORE\"] #\"LN_IC50\",\n",
    "train_processed, stored_lambdas = handle_outliers_and_skewness(train_encoded, numeric_cols, fit_lambda=True)\n",
    "\n",
    "#  Step 2: Process Test Using Stored λ from Train\n",
    "test_processed = handle_outliers_and_skewness(test_encoded, numeric_cols, yeojohnson_lambdas=stored_lambdas, fit_lambda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "jUpWRxXlZ1d9",
    "outputId": "741fc713-49ee-4424-f49d-ce652d2d6031"
   },
   "outputs": [],
   "source": [
    "print(f\"Initial Skewness: {skew(df_id['LN_IC50']):.4f}\")\n",
    "print(f\"Final Skewness: {skew(processed_data['LN_IC50']):.4f}\")\n",
    "print(f\"Skewness difference: {(skew(df_id['LN_IC50']) - skew(processed_data['LN_IC50'])):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't much imporvement in outliers and skewness after applying the various handling methods, beacuse the grouping of drugs per condition already minimizes them. Hence, I go forth without using the previous functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zud_fZY1Cq-P"
   },
   "source": [
    "<h2><b>6.</b> Machine Learning</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvrQ3gIUDTDT"
   },
   "source": [
    "<h3><b>6.1</b> Random Forest</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can always use more parameters in grid search which will slightly improve the result, but for memory reasons I only stick to one set of parameters. The same goes for cross validation folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Xiz9rK4Cqc3",
    "outputId": "b319aa66-cf71-4430-c26b-ff05a2ef4953"
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train = train_encoded.drop('LN_IC50', axis=1)\n",
    "y_train = train_encoded['LN_IC50']\n",
    "X_test = test_encoded.drop('LN_IC50', axis=1)\n",
    "y_test = test_encoded['LN_IC50']\n",
    "Input = [('polynomial', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "         ('scaler', StandardScaler()), ('model', RandomForestRegressor(random_state=42))]\n",
    "pipe = Pipeline(Input)\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100],\n",
    "    'model__max_depth': [None],\n",
    "    'model__min_samples_split': [2],\n",
    "    'model__min_samples_leaf': [1],\n",
    "    'model__max_features': ['sqrt']\n",
    "}\n",
    "gridsearch = GridSearchCV(pipe, param_grid, cv=3, scoring='r2', n_jobs=-1, verbose=1)\n",
    "start_time = time.time()\n",
    "gridsearch.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "time_before = end_time - start_time\n",
    "print(f\"Training time before feature selection: {time_before:.4f} seconds\")\n",
    "print(\"Best parameters:\", gridsearch.best_params_)\n",
    "print(\"Best score:\", gridsearch.best_score_)\n",
    "best_model = gridsearch.best_estimator_\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=kf, scoring='r2', n_jobs=-1)\n",
    "print(f'R^2 scores for each fold: {np.round(cv_scores, 4)}')\n",
    "print(f'Mean R^2 Score: {np.mean(np.round(cv_scores, 4))}')\n",
    "print(f'Standard Deviation of R^2 σ: {np.std(np.round(cv_scores, 4))}')\n",
    "r2_test = best_model.score(X_test, y_test)\n",
    "test_preds = best_model.predict(X_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, test_preds))\n",
    "print(f\"Final Test R^2 score: {r2_test:.4f}\")\n",
    "print(f\"Final Test RMSE: {rmse_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B76bs0qpPHTT",
    "outputId": "55bfbda4-f1cc-4bf3-fd45-e81b9a258ad4"
   },
   "outputs": [],
   "source": [
    "#The same as before but without cross_val_score\n",
    "#As it turns out (obviously) cross_val_score is redundunt,\n",
    "# since GridSearchCV already performs cross validation\n",
    "\n",
    "X_train = train_encoded.drop('LN_IC50', axis=1)\n",
    "y_train = train_encoded['LN_IC50']\n",
    "X_test = test_encoded.drop('LN_IC50', axis=1)\n",
    "y_test = test_encoded['LN_IC50']\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "Input = [('polynomial', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "         ('scaler', StandardScaler()), ('model', RandomForestRegressor(random_state=42))]\n",
    "pipe = Pipeline(Input)\n",
    "param_grid = {\n",
    "    'model__n_estimators': [600],\n",
    "    'model__max_depth': [None],\n",
    "    'model__min_samples_split': [2],\n",
    "    'model__min_samples_leaf': [1],\n",
    "    'model__max_features': ['sqrt']\n",
    "}\n",
    "gridsearch = GridSearchCV(pipe, param_grid, cv=3, scoring='r2', n_jobs=-1, verbose=1)\n",
    "start_time = time.time()\n",
    "gridsearch.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "time_before = end_time - start_time\n",
    "print(f\"Training time before feature selection: {time_before:.4f} seconds\")\n",
    "print(\"Best parameters:\", gridsearch.best_params_)\n",
    "print(\"Best score:\", gridsearch.best_score_)\n",
    "best_model = gridsearch.best_estimator_\n",
    "r2_test = best_model.score(X_test, y_test)\n",
    "test_preds = best_model.predict(X_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, test_preds))\n",
    "print(f\"Final Test R^2 score: {r2_test:.4f}\")\n",
    "print(f\"Final Test RMSE: {rmse_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fULNbiCt-rQH"
   },
   "source": [
    "<h4><b>6.1.1</b> Feature Importance</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "INo5_a9d-qCe",
    "outputId": "aca325b5-fc5a-478b-b9e1-c4d6412e1cc1"
   },
   "outputs": [],
   "source": [
    "feature_importance = best_model.named_steps['model'].feature_importances_\n",
    "feature_names = best_model.named_steps['polynomial'].get_feature_names_out(X_train.columns)\n",
    "print(f\"Length of feature importance: {len(feature_importance)}\")\n",
    "print(f\"Length of feature names: {len(feature_names)}\")\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "plt.figure(figsize=(25, 15))\n",
    "sns.barplot(x='Importance', y='Feature', hue='Feature', data=feature_importance_df, palette='viridis')\n",
    "plt.title('Random Forest Feature Importance', fontsize=16)\n",
    "plt.xlabel('Importance', fontsize=14)\n",
    "plt.ylabel('Feature', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byqMBwuFHc5J"
   },
   "source": [
    "<h4><b>6.1.2</b> Remove Low-Importance features and retrain</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RAlHD0ruEjpg",
    "outputId": "4b128264-fe25-4a71-e97c-3c32eae06c75"
   },
   "outputs": [],
   "source": [
    "del df, df_id, train_encoded, test_encoded, filtered_df_LAML, filtered_df_UNCLASSIFIED, target_unique_counts\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7tmfZVreMEhJ",
    "outputId": "a0720fe8-64a2-4049-fd7f-b7011bf9c0be"
   },
   "outputs": [],
   "source": [
    "#  Apply PolynomialFeatures (Keep degree=2 for fair comparison)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# Get Correct Feature Names\n",
    "feature_names_poly = poly.get_feature_names_out(X_train.columns)\n",
    "\n",
    "# Convert to DataFrame\n",
    "X_train_poly_df = pd.DataFrame(X_train_poly, columns=feature_names_poly, index=X_train.index)\n",
    "X_test_poly_df = pd.DataFrame(X_test_poly, columns=feature_names_poly, index=X_test.index)\n",
    "\n",
    "#  Measure Training Time BEFORE Feature Selection.\n",
    "# Not enough memory to perform the fit again! I'll use the time_before from the previous run\n",
    "#start_time = time.time()\n",
    "#best_model.fit(X_train_poly_df, y_train)  # Train with ALL features\n",
    "#end_time = time.time()\n",
    "#time_before = end_time - start_time\n",
    "#print(f\" Training Time BEFORE Feature Selection: {time_before:.4f} seconds\")\n",
    "\n",
    "#  Remove Low-Importance Features (AFTER PolynomialFeatures)\n",
    "importance_threshold = 0.001  # Adjust as needed\n",
    "low_importance_features = feature_importance_df[feature_importance_df[\"Importance\"] < importance_threshold][\"Feature\"].tolist()\n",
    "\n",
    "X_train_filtered = X_train_poly_df.drop(columns=low_importance_features)\n",
    "X_test_filtered = X_test_poly_df.drop(columns=low_importance_features)\n",
    "\n",
    "# Measure Training Time AFTER Feature Selection\n",
    "start_time = time.time()\n",
    "best_model.fit(X_train_filtered, y_train)  # Train with REDUCED features\n",
    "end_time = time.time()\n",
    "time_after = end_time - start_time\n",
    "print(f\"Training Time AFTER Feature Selection: {time_after:.4f} seconds\")\n",
    "\n",
    "#  Compare Time Reduction\n",
    "time_reduction = ((time_before - time_after) / time_before) * 100\n",
    "print(f\"Time Reduction: {time_reduction:.2f}%\")\n",
    "\n",
    "#  Evaluate on Test Set (AFTER Feature Selection)\n",
    "r2_test_filtered = best_model.score(X_test_filtered, y_test)\n",
    "test_preds_filtered = best_model.predict(X_test_filtered)\n",
    "rmse_test_filtered = np.sqrt(mean_squared_error(y_test, test_preds_filtered))\n",
    "\n",
    "print(f\"Final Test R² After Feature Selection: {r2_test_filtered:.4f}\")\n",
    "print(f\"Final Test RMSE After Feature Selection: {rmse_test_filtered:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6ce1P5f2yrG"
   },
   "source": [
    ":There is a slight accuracy improvement, but much worse time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X2cLCYKr5KJy",
    "outputId": "de690593-ea06-4853-f3c8-04c1a5bf73f1"
   },
   "outputs": [],
   "source": [
    "del X_train_poly, X_test_poly, X_train_poly_df, X_test_poly_df, X_train_filtered, X_test_filtered\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_eyhNeqYT7M"
   },
   "source": [
    "Let's try SnapML to speed up the calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>6.2</b> SnapML</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbUVg3cH3EE-",
    "outputId": "31665e3a-748c-4d82-d916-cdf5db6585ff"
   },
   "outputs": [],
   "source": [
    "#SnapML\n",
    "X_train = train_encoded.drop('LN_IC50', axis=1)\n",
    "y_train = train_encoded['LN_IC50']\n",
    "X_test = test_encoded.drop('LN_IC50', axis=1)\n",
    "y_test = test_encoded['LN_IC50']\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "Input = [\n",
    "    ('polynomial', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', SnapRandomForestRegressor(random_state=42))\n",
    "]\n",
    "pipe = Pipeline(Input)\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100],\n",
    "    'model__max_depth': [None],\n",
    "    'model__min_samples_leaf': [1],\n",
    "    'model__max_features': ['sqrt']\n",
    "}\n",
    "gridsearch = GridSearchCV(pipe, param_grid, cv=3, scoring='r2', n_jobs=-1, verbose=1)\n",
    "start_time = time.time()\n",
    "gridsearch.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "time_before = end_time - start_time\n",
    "print(f\"Training time before feature selection with SnapML: {time_before:.4f} seconds\")\n",
    "print(\"Best parameters SnapML:\", gridsearch.best_params_)\n",
    "print(\"Best score SnapML:\", gridsearch.best_score_)\n",
    "best_model = gridsearch.best_estimator_\n",
    "r2_test = best_model.score(X_test, y_test)\n",
    "test_preds = best_model.predict(X_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, test_preds))\n",
    "print(f\"Final Test R^2 score with SnapML: {r2_test:.4f}\")\n",
    "print(f\"Final Test RMSE with SnapML: {rmse_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>6.3</b> Decision Tree</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_encoded.drop('LN_IC50', axis=1)\n",
    "y_train = train_encoded['LN_IC50']\n",
    "X_test = test_encoded.drop('LN_IC50', axis=1)\n",
    "y_test = test_encoded['LN_IC50']\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "Input = [\n",
    "    ('polynomial', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', DecisionTreeRegressor(random_state=42))\n",
    "]\n",
    "pipe = Pipeline(Input)\n",
    "param_grid = {\n",
    "    'model__max_depth': [None],\n",
    "    'model__min_samples_split': [2],\n",
    "    'model__min_samples_leaf': [1],\n",
    "    'model__max_features': ['sqrt']\n",
    "}\n",
    "gridsearch = GridSearchCV(pipe, param_grid, cv=3, scoring='r2', n_jobs=-1, verbose=1)\n",
    "\n",
    "start_time = time.time()\n",
    "gridsearch.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "time_before = end_time - start_time\n",
    "print(f\"Training time before feature selection with Decision Tree: {time_before:.4f} seconds\")\n",
    "best_model = gridsearch.best_estimator_\n",
    "print(\"Best parameters:\", gridsearch.best_params_)\n",
    "print(\"Best train R^2 score:\", gridsearch.best_score_)\n",
    "\n",
    "test_preds = best_model.predict(X_test)\n",
    "r2_test = r2_score(y_test, test_preds)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, test_preds))\n",
    "\n",
    "print(f\"Final Test R^2 score: {r2_test:.4f}\")\n",
    "print(f\"Final Test RMSE: {rmse_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The faster, but not as good as SnapML. Also, using feature importance improved the R^2 score slightly from 0.9757 to 0.9875."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>6.4</b> Linear Regression</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>6.4.1</b> Polynomial Regression</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_encoded.drop('LN_IC50', axis=1)\n",
    "y_train = train_encoded['LN_IC50']\n",
    "X_test = test_encoded.drop('LN_IC50', axis=1)\n",
    "y_test = test_encoded['LN_IC50']\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "Input = [\n",
    "        ('polynomial', PolynomialFeatures(include_bias=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', LinearRegression())\n",
    "        ]\n",
    "\n",
    "pipe = Pipeline(Input)\n",
    "param_grid = {'polynomial__degree':[2, 3]}\n",
    "gridsearch = GridSearchCV(pipe, param_grid, cv=3, scoring='r2', n_jobs=-1, verbose=1)\n",
    "\n",
    "start_time = time.time()\n",
    "gridsearch.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "time_before = end_time - start_time\n",
    "print(f\"Training time before feature selection with Multi-Linear Polynomial Regrssion: {time_before:.4f} seconds\")\n",
    "best_model = gridsearch.best_estimator_\n",
    "print(\"Best parameters:\", gridsearch.best_params_)\n",
    "print(\"Best train R^2 score:\", gridsearch.best_score_)\n",
    "\n",
    "test_preds = best_model.predict(X_test)\n",
    "r2_test = r2_score(y_test, test_preds)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, test_preds))\n",
    "\n",
    "print(f\"Final Test R^2 score: {r2_test:.4f}\")\n",
    "print(f\"Final Test RMSE: {rmse_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected for a non-linear dataframe, Linear Regression is much worse than Trees, even with Polynomial Features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>6.4.2</b> Ridge Regression</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_encoded.drop('LN_IC50', axis=1)\n",
    "y_train = train_encoded['LN_IC50']\n",
    "X_test = test_encoded.drop('LN_IC50', axis=1)\n",
    "y_test = test_encoded['LN_IC50']\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "Input = [\n",
    "        ('polynomial', PolynomialFeatures(include_bias=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', Ridge())\n",
    "        ]\n",
    "param_grid = {'polynomial__degree':[2, 3],\n",
    "              'model__alpha': [0.01, 0.1, 1, 10, 100]\n",
    "              }\n",
    "pipe = Pipeline(Input)\n",
    "gridsearch = GridSearchCV(pipe, param_grid, cv=3, scoring='r2', n_jobs=-1, verbose=1)\n",
    "\n",
    "start_time = time.time()\n",
    "gridsearch.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "time_before = end_time - start_time\n",
    "print(f\"Training time before feature selection with Ridge Regression: {time_before:.4f} seconds\")\n",
    "\n",
    "best_model = gridsearch.best_estimator_\n",
    "print(\"Best parameters:\", gridsearch.best_params_)\n",
    "print(\"Best train R^2 score:\", gridsearch.best_score_)\n",
    "\n",
    "test_preds = best_model.predict(X_test)\n",
    "r2_test = r2_score(y_test, test_preds)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, test_preds))\n",
    "\n",
    "print(f\"Final Test R^2 score: {r2_test:.4f}\")\n",
    "print(f\"Final Test RMSE: {rmse_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small difference from Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>6.4.3</b> Lasso Regression</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_encoded.drop('LN_IC50', axis=1)\n",
    "y_train = train_encoded['LN_IC50']\n",
    "X_test = test_encoded.drop('LN_IC50', axis=1)\n",
    "y_test = test_encoded['LN_IC50']\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "Input = [\n",
    "        ('polynomial', PolynomialFeatures(include_bias=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', Lasso())\n",
    "        ]\n",
    "pipe = Pipeline(Input)\n",
    "param_grid = {'polynomial__degree':[2, 3],\n",
    "              'model__alpha': [0.01, 0.1, 1, 10]\n",
    "              }\n",
    "gridsearch = GridSearchCV(pipe, param_grid, cv=3, scoring='r2', n_jobs=-1, verbose=1)\n",
    "start_time = time.time()\n",
    "gridsearch.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "time_before = end_time - start_time\n",
    "print(f\"Training time before feature selection with Lasso Regression: {time_before:.4f} seconds\")\n",
    "best_model = gridsearch.best_estimator_\n",
    "print(\"Best parameters:\", gridsearch.best_params_)\n",
    "print(\"Best train R^2 score:\", gridsearch.best_score_)\n",
    "test_preds = best_model.predict(X_test)\n",
    "r2_test = r2_score(y_test, test_preds)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, test_preds))\n",
    "print(f\"Final Test R^2 score: {r2_test:.4f}\")\n",
    "print(f\"Final Test RMSE: {rmse_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence all the Linear models, with regulirization or not, perform less than non-linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try SVR now, which is great for high-dimensional, non-linear data and is less sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>6.5</b> SVR</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_encoded.drop('LN_IC50', axis=1)\n",
    "y_train = train_encoded['LN_IC50']\n",
    "X_test = test_encoded.drop('LN_IC50', axis=1)\n",
    "y_test = test_encoded['LN_IC50']\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "Input = [\n",
    "        ('polynomial', PolynomialFeatures(include_bias=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', SVR())\n",
    "        ]\n",
    "pipe = Pipeline(Input)\n",
    "param_grid = {'polynomial__degree':[2],\n",
    "              'model__C': [ 0.1, 1],\n",
    "              'model__kernel': [\"rbf\"],\n",
    "              'model__epsilon': [0.1, 1]\n",
    "              }\n",
    "gridsearch = GridSearchCV(pipe, param_grid, cv=3, scoring='r2', n_jobs=-1, verbose=1)\n",
    "start_time = time.time()\n",
    "gridsearch.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "time_before = end_time - start_time\n",
    "print(f\"Training time before feature selection with SVR: {time_before:.4f} seconds\")\n",
    "best_model = gridsearch.best_estimator_\n",
    "print(\"Best parameters:\", gridsearch.best_params_)\n",
    "print(\"Best train R^2 score:\", gridsearch.best_score_)\n",
    "test_preds = best_model.predict(X_test)\n",
    "r2_test = r2_score(y_test, test_preds)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, test_preds))\n",
    "print(f\"Final Test R^2 score: {r2_test:.4f}\")\n",
    "print(f\"Final Test RMSE: {rmse_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Random Forest is the best performing up to now, but relatively slow, I'll try XGBoost which is faster and more accurate, especially for large data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>6.6</b> XGBoost</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_encoded.drop('LN_IC50', axis=1)\n",
    "y_train = train_encoded['LN_IC50']\n",
    "X_test = test_encoded.drop('LN_IC50', axis=1)\n",
    "y_test = test_encoded['LN_IC50']\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "Input = [\n",
    "        ('polynomial', PolynomialFeatures(include_bias=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', XGBRegressor(random_state=42))\n",
    "]\n",
    "pipe = Pipeline(Input)\n",
    "param_grid = {\n",
    "    'model__n_estimators': [1000],\n",
    "    'model__max_depth': [ None],\n",
    "    'model__learning_rate': [ 0.1],\n",
    "    'model__subsample': [ 1.0],\n",
    "    'model__colsample_bytree': [1.0]\n",
    "  }\n",
    "\n",
    "gridsearch = GridSearchCV(pipe, param_grid, cv=3, scoring='r2', n_jobs=-1, verbose=1)\n",
    "start_time = time.time()\n",
    "gridsearch.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "time_before = end_time - start_time\n",
    "print(f\"Training time before feature selection with XGBoost: {time_before:.4f} seconds\")\n",
    "best_model = gridsearch.best_estimator_\n",
    "print(\"Best parameters:\", gridsearch.best_params_)\n",
    "print(\"Best train R^2 score:\", gridsearch.best_score_)\n",
    "test_preds = best_model.predict(X_test)\n",
    "r2_test = r2_score(y_test, test_preds)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, test_preds))\n",
    "print(f\"Final Test R^2 score: {r2_test:.4f}\")\n",
    "print(f\"Final Test RMSE: {rmse_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best combination of accuracy and time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since XGBoost is the best model since now I'll try the corresponding model from SnapML which will reasonably will give the same accuracy with faster training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>6.7</b> SBMRegression</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_encoded.drop('LN_IC50', axis=1)\n",
    "y_train = train_encoded['LN_IC50']\n",
    "X_test = test_encoded.drop('LN_IC50', axis=1)\n",
    "y_test = test_encoded['LN_IC50']\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "Input = [\n",
    "        ('polynomial', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', SnapBoostingMachineRegressor(random_state=42))\n",
    "]\n",
    "pipe = Pipeline(Input)\n",
    "param_grid = {\n",
    "     'model__max_depth': [10],\n",
    "    'model__learning_rate': [0.1],\n",
    "    'model__subsample': [ 0.8],\n",
    "    'model__colsample_bytree': [ 0.8],\n",
    "}\n",
    "gridsearch = GridSearchCV(pipe, param_grid, cv=3, scoring='r2', n_jobs=-1, verbose=1)\n",
    "start_time = time.time()\n",
    "gridsearch.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "time_before = end_time - start_time\n",
    "print(f\"Training time before feature selection with SnapML: {time_before:.4f} seconds\")\n",
    "best_model = gridsearch.best_estimator_\n",
    "print(\"Best parameters:\", gridsearch.best_params_)\n",
    "print(\"Best train R^2 score:\", gridsearch.best_score_)\n",
    "test_preds = best_model.predict(X_test)\n",
    "r2_test = r2_score(y_test, test_preds)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, test_preds))\n",
    "print(f\"Final Test R^2 score: {r2_test:.4f}\")\n",
    "print(f\"Final Test RMSE: {rmse_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM is another reasonable choice since XGBoost performed very well, but we want faster times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>6.8</b> LightGBM</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_encoded.drop('LN_IC50', axis=1)\n",
    "y_train = train_encoded['LN_IC50']\n",
    "X_test = test_encoded.drop('LN_IC50', axis=1)\n",
    "y_test = test_encoded['LN_IC50']\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "Input = [\n",
    "        ('polynomial', PolynomialFeatures(include_bias=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', LGBMRegressor(random_state=42))\n",
    "]\n",
    "pipe = Pipeline(Input)\n",
    "param_grid = {\n",
    "    'model__n_estimators': [1000],\n",
    "    'model__max_depth': [None],\n",
    "    'model__learning_rate': [0.1, 0.4],\n",
    "    'model__subsample': [0.2]\n",
    "}\n",
    "gridsearch = GridSearchCV(pipe, param_grid, cv=3, scoring=\"r2\", n_jobs=-1, verbose=1)\n",
    "start_time = time.time()\n",
    "gridsearch.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "time_before = end_time - start_time\n",
    "print(f\"Training time before feature selection with LightGBM: {time_before:.4f} seconds\")\n",
    "best_model = gridsearch.best_estimator_\n",
    "print(\"Best parameters:\", gridsearch.best_params_)\n",
    "print(\"Best train R^2 score:\", gridsearch.best_score_)\n",
    "test_preds = best_model.predict(X_test)\n",
    "r2_test = r2_score(y_test, test_preds)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, test_preds))\n",
    "print(f\"Final Test R^2 score: {r2_test:.4f}\")\n",
    "print(f\"Final Test RMSE: {rmse_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try KNN regression, although it's slow for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>6.9</b> KNN Regression</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_encoded.drop('LN_IC50', axis=1)\n",
    "y_train = train_encoded['LN_IC50']\n",
    "X_test = test_encoded.drop('LN_IC50', axis=1)\n",
    "y_test = test_encoded['LN_IC50']\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "Input = [\n",
    "        ('polynomial', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', KNeighborsRegressor())\n",
    "        ]\n",
    "pipe = Pipeline(Input)\n",
    "param_grid = {\n",
    "    'model__n_neighbors': [3, 5, 7, 10],\n",
    "    'model__weights': ['uniform', 'distance'],\n",
    "    'model__p': [1, 2],\n",
    "    'model__metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "gridsearch = GridSearchCV(pipe, param_grid, cv=3, scoring='r2', n_jobs=-1, verbose=1)\n",
    "start_time = time.time()\n",
    "gridsearch.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "time_before = end_time - start_time\n",
    "print(f\"Training time before feature selection with KNN: {time_before:.4f} seconds\")\n",
    "best_model = gridsearch.best_estimator_\n",
    "print(\"Best parameters:\", gridsearch.best_params_)\n",
    "print(\"Best train R^2 score:\", gridsearch.best_score_)\n",
    "test_preds = best_model.predict(X_test)\n",
    "r2_test = r2_score(y_test, test_preds)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, test_preds))\n",
    "print(f\"Final Test R^2 score: {r2_test:.4f}\")\n",
    "print(f\"Final Test RMSE: {rmse_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have non-linear relationships let's also try Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>6.10</b> MLPRegressor</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_encoded.drop('LN_IC50', axis=1)\n",
    "y_train = train_encoded['LN_IC50']\n",
    "X_test = test_encoded.drop('LN_IC50', axis=1)\n",
    "y_test = test_encoded['LN_IC50']\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "Input = [\n",
    "        ('polynomial', PolynomialFeatures(include_bias=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', MLPRegressor(random_state=42))\n",
    "        ]\n",
    "pipe = Pipeline(Input)\n",
    "parma_grid = {\n",
    "    'model__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'model__activation': ['relu', 'tanh'],\n",
    "    'model__solver': ['adam', 'sgd'],\n",
    "    'model__alpha': [0.0001, 0.001, 0.01],\n",
    "    'model__learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "gridsearch = GridSearchCV(pipe, parma_grid, cv=3, scoring='r2', n_jobs=-1, verbose=1)\n",
    "start_time = time.time()\n",
    "gridsearch.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "time_before = end_time - start_time\n",
    "print(f\"Training time before feature selection with MLPRegressor: {time_before:.4f} seconds\")\n",
    "best_model = gridsearch.best_estimator_\n",
    "print(\"Best parameters:\", gridsearch.best_params_)\n",
    "print(\"Best train R^2 score:\", gridsearch.best_score_)\n",
    "test_preds = best_model.predict(X_test)\n",
    "r2_test = r2_score(y_test, test_preds)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, test_preds))\n",
    "print(f\"Final Test R^2 score: {r2_test:.4f}\")\n",
    "print(f\"Final Test RMSE: {rmse_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>7.</b> Conclusion</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "After data importing, preprocessing, encoding, EDA etc the best performing models were XGBoost and LightGBM, with a test accuracy of 0.999. Obviously, with more memory you can do a wider grid search, more cv folds etc and achieve an even better score. Decision trees was the fastest, but with less accuracy. Given the magnitude and non-linearity of the data set, the choice of random forest-like models was apparent. Following Samira Alipour while checking for outliers and skewness I grouped the dataset on drugs. However, in my approach, I also grouped for drugs per specific tissue and condition, which narrowed down the the range of outliers and skewness. In fact, the skewness was that low which dictated that I shouldn't move forward in handling it, since I would loose valuable info (I include the code for handling outliers and skewnees, even though I don't use it).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "I hope you find this interesting and helpfull.\n",
    "\n",
    "Thank you!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ektoras Delaportas  \n",
    "Chemist MSc, Pharmacist MPharm"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
